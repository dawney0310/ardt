import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm
from scipy.optimize import linprog
import gym

from data_loading.load_mujoco import Trajectory
from return_transforms.models.ardt.maxmin_model import RtgFFN, RtgLSTM
from return_transforms.datasets.ardt_dataset import ARDTDataset


# 1. Solve CE via linear programming
def solve_ce_lp_batch(q_values: torch.Tensor) -> torch.Tensor:
    B, T, A1, A2 = q_values.shape
    ce_policies = []

    for b in range(B):
        batch_policies = []
        for t in range(T):
            Q = q_values[b, t].detach().cpu().numpy()
            n = A1 * A2
            c = -Q.flatten()

            A = []
            b_vec = []

            # Player 1 constraints
            for a1 in range(A1):
                for a1p in range(A1):
                    if a1 == a1p: continue
                    row = np.zeros((A1, A2))
                    row[a1, :] = Q[a1, :]
                    row[a1p, :] -= Q[a1p, :]
                    A.append(row.flatten())
                    b_vec.append(0)

            # Player 2 constraints
            for a2 in range(A2):
                for a2p in range(A2):
                    if a2 == a2p: continue
                    row = np.zeros((A1, A2))
                    row[:, a2] = Q[:, a2]
                    row[:, a2p] -= Q[:, a2p]
                    A.append(row.flatten())
                    b_vec.append(0)

            A_eq = [np.ones(n)]
            b_eq = [1.0]
            bounds = [(0, 1)] * n

            res = linprog(c, A_ub=A, b_ub=b_vec, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
            if res.success:
                pi = res.x.reshape((A1, A2))
            else:
                pi = np.ones((A1, A2)) / (A1 * A2)

            batch_policies.append(torch.tensor(pi, dtype=torch.float32))
        ce_policies.append(torch.stack(batch_policies))

    return torch.stack(ce_policies)  # [B, T, A1, A2]


# 2. Train CE-Q using LP-solved CE policy
def maxmin(
    trajs: list[Trajectory],
    action_space: gym.spaces.Space,
    adv_action_space: gym.spaces.Space,
    train_args: dict,
    device: str,
    n_cpu: int,
    is_simple_model: bool = False,
    is_toy: bool = False,
    is_discretize: bool = False,
):
    # 1️⃣ Determine action type: discrete or continuous
    if isinstance(action_space, gym.spaces.Discrete):
        obs_size = np.prod(trajs[0].obs[0].shape)
        action_size = action_space.n
        adv_action_size = adv_action_space.n
        action_type = 'discrete'
    else:
        obs_size = np.prod(np.array(trajs[0].obs[0]).shape)
        action_size = action_space.shape[0]
        adv_action_size = adv_action_space.shape[0]
        action_type = 'continuous'

    # 2️⃣ Build dataset and dataloader from offline trajectories
    max_len = max([len(traj.obs) for traj in trajs]) + 1
    dataset = ARDTDataset(
        trajs,
        action_size,
        adv_action_size,
        max_len,
        gamma=train_args['gamma'],
        act_type=action_type
    )
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=train_args['batch_size'],
        num_workers=n_cpu
    )

    # 3️⃣ Instantiate models (either simple FFN or LSTM-based ARDT models)
    print(f'Creating models... (simple={is_simple_model})')
    if is_simple_model:
        qsa_pr_model = RtgFFN(obs_size, action_size, include_adv=False).to(device)
        qsa_adv_model = RtgFFN(obs_size, action_size, adv_action_size, include_adv=True).to(device)
    else:
        qsa_pr_model = RtgLSTM(obs_size, action_size, adv_action_size, train_args, include_adv=False).to(device)
        qsa_adv_model = RtgLSTM(obs_size, action_size, adv_action_size, train_args, include_adv=True).to(device)

    # ➡️ Training logic (MSE warm-up / CE-Q learning) follows here...

    dataloader = DataLoader(dataset, batch_size=train_args['batch_size'], num_workers=0)
    optimizer_pr = torch.optim.AdamW(qsa_pr_model.parameters(), lr=train_args['model_lr'], weight_decay=train_args['model_wd'])
    optimizer_adv = torch.optim.AdamW(qsa_adv_model.parameters(), lr=train_args['model_lr'], weight_decay=train_args['model_wd'])


    mse_epochs = train_args['mse_epochs']
    maxmin_epochs = train_args['maxmin_epochs']
    total_epochs = mse_epochs + maxmin_epochs

    for epoch in range(total_epochs):
        pbar = tqdm(dataloader)
        total_loss = 0
        total_pr_loss = 0
        total_adv_loss = 0
        total_batches = 0

        for obs, acts, adv_acts, ret, _ in pbar:
            B, T = obs.shape[:2]
            obs = obs.view(B, T, -1).to(device)
            acts = acts.to(device)
            adv_acts = adv_acts.to(device)
            ret = (ret / train_args['scale']).to(device)

            total_batches += 1
            optimizer_pr.zero_grad()
            optimizer_adv.zero_grad()

            if epoch < mse_epochs:
                ret_adv_pred = qsa_adv_model(obs, acts, adv_acts).view(B, T)
                ret_pr_pred = qsa_pr_model(obs, acts).view(B, T)
                ret_pr_loss = F.mse_loss(ret_pr_pred, ret)
                ret_adv_loss = F.mse_loss(ret_adv_pred, ret)


                ret_pr_loss.backward()
                ret_adv_loss.backward()
                optimizer_pr.step()
                optimizer_adv.step()

                total_loss += ret_pr_loss.item() + ret_adv_loss.item()
                total_pr_loss += ret_pr_loss.item()
                total_adv_loss += ret_adv_loss.item()

            else:
                N = action_size * adv_action_size
                joint = torch.cartesian_prod(torch.arange(action_size), torch.arange(adv_action_size)).to(device)
                a_joint = F.one_hot(joint[:, 0], action_size).float()
                adv_joint = F.one_hot(joint[:, 1], adv_action_size).float()

                obs_exp = obs.unsqueeze(2).expand(B, T, N, obs.shape[-1])
                a_exp = a_joint.view(1, 1, N, action_size).expand(B, T, N, action_size)
                adv_exp = adv_joint.view(1, 1, N, adv_action_size).expand(B, T, N, adv_action_size)

                q_all = qsa_adv_model(
                    obs_exp.reshape(B * T * N, 1, obs.shape[-1]),
                    a_exp.reshape(B * T * N, 1, action_size),
                    adv_exp.reshape(B * T * N, 1, adv_action_size)
                ).view(B, T, action_size, adv_action_size)

                ce_policy = solve_ce_lp_batch(q_all).to(device)
                q_ce_target = (ce_policy * q_all).sum(dim=[2, 3])

                ret_adv_pred = qsa_adv_model(obs, acts, adv_acts).view(B, T)
                ret_pr_pred = qsa_pr_model(obs, acts).view(B, T)

                ret_adv_loss = F.mse_loss(ret_adv_pred, q_ce_target.detach())
                ret_pr_loss = F.mse_loss(ret_pr_pred, q_ce_target.detach())


                total_loss += ret_pr_loss.item() + ret_adv_loss.item()
                total_pr_loss += ret_pr_loss.item()
                total_adv_loss += ret_adv_loss.item()

                ret_adv_loss.backward()
                ret_pr_loss.backward()
                optimizer_adv.step()
                optimizer_pr.step()

            pbar.set_description(
                f"Epoch {epoch} | "
                f"Total Loss: {total_loss / total_batches:.4f} | "
                f"Pr Loss: {total_pr_loss / total_batches:.4f} | "
                f"Adv Loss: {total_adv_loss / total_batches:.4f}"
            )
        

    # Evaluation (relabel return-to-go)
    with torch.no_grad():
        learned_returns = []
        prompt_value = -np.inf

        for traj in tqdm(trajs):
            obs = torch.from_numpy(np.array(traj.obs)).float().to(device).view(1, -1, obs_size)
            acts = torch.from_numpy(np.array(traj.actions)).long().to(device).view(1, -1)
            if action_type == "discrete" and not is_discretize:
                acts = torch.nn.functional.one_hot(acts, num_classes=action_size)
            else:
                acts = acts.view(1, -1, action_size)

            q_preds = qsa_pr_model(obs.view(obs.shape[0], -1, obs_size), acts.float()).cpu().flatten().numpy()
            prompt_value = max(prompt_value, q_preds[-len(traj.actions)])
            learned_returns.append(np.round(q_preds * train_args['scale'], 3))

    return learned_returns, np.round(prompt_value * train_args['scale'], 3)
